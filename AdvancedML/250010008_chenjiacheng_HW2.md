# Assignment 2: Data classification using SVM

**姓名**: 陈嘉诚
**学号**: 250010008


## 1. 目标

支持向量机 (SVM) 是一种经典的监督学习算法，广泛用于分类和回归任务。本作业的目标是在不同的数据集上实现和分析SVM的几种变体。

---

## 2. 任务与分析

### Problem 1: 线性分类基础

**任务**：在乳腺癌 (Breast Cancer) 数据集上实现一个线性核SVM分类器。

#### 数据预处理与模型训练程序

我们的流程严格遵循了任务要求：

1.  **数据集**：我们使用了 Scikit-learn 自带的 "Breast Cancer Wisconsin (Diagnostic)" 数据集。
2.  **数据划分**：将数据集按照 80% 训练集和 20% 测试集的比例进行划分，并采用分层抽样 (stratify) 以确保训练集和测试集中良性/恶性肿瘤的比例一致。
3.  **数据预处理**：SVM 算法对特征的尺度非常敏感。因此，我们使用了 `StandardScaler` (一种特征缩放技术) 对数据进行标准化处理。我们首先在训练集上 `fit` 缩放器，然后用它分别 `transform` 训练集和测试集。
4.  **模型训练**：我们实例化了一个 `SVC` (Support Vector Classifier) 模型，并将其 `kernel` 超参数设置为 `'linear'`，其余超参数（如 `C=1.0`）保持默认设置。

#### 结果解释

**模型性能：**

```text
线性 SVM 在测试集上的准确率: 95.61%
分类报告:
              precision    recall  f1-score   support
   malignant       0.95      0.93      0.94        42
      benign       0.96      0.97      0.97        72
    accuracy                           0.96       114
   macro avg       0.96      0.95      0.95       114
weighted avg       0.96      0.96      0.96       114
````

**结论**：线性核 SVM 在测试集上达到了 **95.61%** 的高准确率。这有力地表明，乳腺癌数据集在很大程度上是线性可分的。这意味着一个线性的超平面（决策边界）就足以在多维特征空间中很好地将恶性肿瘤和良性肿瘤区分开来。

-----

### Problem 2: 不同核函数的比较

**任务**：在乳腺癌数据集上训练和比较线性核 (linear)、多项式核 (polynomial) 和高斯RBF核 (Gaussian RBF)。

#### 不同核函数及参数选择对准确率的影响

为了系统地比较核函数并优化它们的参数，我们使用了 `GridSearchCV`（5折交叉验证）来搜索最佳模型。

  * **搜索空间**：

      * `linear` 核：`C` in `[0.1, 1, 10]`
      * `poly` 核：`C` in `[1, 10]`, `degree` in `[2, 3]`
      * `rbf` 核：`C` in `[1, 10, 100]`, `gamma` in `['scale', 0.1, 0.01]`

  * **GridSearchCV 结果**：

      * **最佳参数**：`{'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}`
      * **最佳交叉验证准确率**：98.02%
      * **最佳模型在测试集上的准确率**：98.25%

**影响总结**：

1.  **线性核 (Linear)**：作为基线，表现已非常出色 (约 95.6%)。
2.  **多项式核 (Poly)**：可以模拟更复杂的曲线边界。它的表现高度依赖于 `degree`（多项式次数）和 `C`（正则化）的选择。
3.  **高斯RBF核 (RBF)**：在此次实验中表现最好（测试准确率 98.25%）。这表明尽管数据 *大部分* 是线性可分的，但仍然存在一些非线性关系，RBF 核能够灵活地捕捉到这些关系，从而构建一个更精确的决策边界。
4.  **参数选择**：`C` (正则化参数) 和 `gamma` (RBF/Poly 核系数) 对非线性核的性能至关重要，调参不当会导致过拟合或欠拟合。

#### 决策边界的合理性

最佳模型（RBF核, C=1, gamma='scale'）在未见过的测试集上达到了 98.25% 的高准确率。这证明了其决策边界的**合理性**和**强大的泛化能力**。

它没有因为拟合训练集中的噪声而变得过于复杂（过拟合），而是成功地学习到了良性/恶性类别之间的真正潜在模式。RBF核提供的非线性能力在这里是“恰到好处”的，它对线性边界进行了微调，从而正确分类了那些线性核无法处理的、靠近边界的样本。

-----

### Problem 3: 解决更复杂的问题 (MNIST)

**任务**：在 MNIST 数据集上进行 SVM 实验。

#### 实验策略（包含额外技术）

MNIST 数据集非常大（70,000个 28x28 像素的图像），直接在完整数据集上训练 SVM 计算上是极其昂贵的。因此，我们采用了一种多阶段的“适当策略”：

1.  **子集采样 (Subsampling)**：我们从完整数据集中随机抽取了一个子集（10,000个训练样本，2,000个测试样本）来进行实验。
2.  **特征缩放 (Feature Scaling)**：我们使用 `StandardScaler` 对像素值（0-255）进行标准化，这是作业中提到的推荐技术之一。
3.  **主成分分析 (PCA)**：我们应用 PCA 来进行降维，保留 95% 的原始方差。这成功地将特征维度从 784 维降低到了 154 维，极大地加速了 SVM 的训练过程。
4.  **模型选择**：鉴于图像分类的复杂非线性特性，我们选择了 **RBF 核**进行训练，这属于一种核优化策略。

**实验结果**：
使用 (RBF核 + PCA + Scaling) 的策略，在 2,000 个测试样本上，模型达到了 **97.70%** 的准确率。

#### 决策边界最模糊的两个类别

“决策边界模糊”意味着模型最容易将这两个类别混淆。我们通过分析测试集的**混淆矩阵 (Confusion Matrix)** 来找出这一点。

通过检查混淆矩阵中非对角线上的值（即误分类的次数），我们发现了最常发生的误分类：

1.  **Top 1 混淆**：**9** 被预测为 **4** (10 次)
2.  **Top 2 混淆**：**9** 被预测为 **7** (8 次)

**结论**：
根据本次实验，决策边界最模糊（最常被混淆）的两个类别是 **9 和 4**。

其次是 9 和 7。这在直觉上是合理的，因为它们的手写形态在视觉上非常相似。

-----
